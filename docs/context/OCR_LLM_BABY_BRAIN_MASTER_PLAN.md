# OCR/LLM "Baby Brain" Master Plan (Add Cards → KingsReview)

## Big-Picture Vision
Build the card-intake brain so one high-quality human teach action on a card teaches the system the set family instantly:
- If the model is confident, it auto-fills correctly.
- If the model is not confident, it says "I don't know" (no forced bad guesses).
- When a human corrects one card (base or insert/parallel) and trains it, the next similar cards from the same set/year/manufacturer are recognized and suggested correctly.
- The learning is persisted in our own DB (not ephemeral), auditable, and improves week over week.

In plain terms: one teach should create reusable memory for "cards like this" and that memory should be used immediately on the next card.

## Why This Matters
1. Throughput: reduce manual edits in Add Cards.
2. Accuracy: avoid wrong set drift and wrong parallel defaults.
3. Trust: operators should see predictable, explainable suggestions.
4. Flywheel: every reviewed card improves future automation.

## Current Reality (as of 2026-02-23)
1. OCR provider: Google Vision `DOCUMENT_TEXT_DETECTION` on FRONT/BACK/TILT.
2. LLM parser: OpenAI Responses API with `OCR_LLM_MODEL` (`gpt-5`) + fallback (`gpt-5-mini`).
3. Teach memory exists via `OcrFeedbackEvent` and is replayed during OCR suggestion.
4. Remaining pain points seen in prod testing:
- set misses / wrong set bias in some flows
- one-off non-canonical option behavior in picker history
- noisy parallel fallbacks (historically color over-bias)

## Product Principles (Hard Rules)
1. Never force a confident guess when evidence is weak.
2. Constrain LLM selection to DB candidates whenever possible.
3. Keep human override fast and obvious.
4. Persist every teach event with enough evidence to replay safely.
5. Evaluate continuously with production-like test sets.

## Target Architecture
1. Per-photo OCR extraction (FRONT/BACK/TILT).
2. Candidate generation from DB (approved sets first, then set-scoped inserts/parallels).
3. Hybrid LLM decision:
- constrained taxonomy fields: `setName`, `insertSet`, `parallel`
- free-text OCR+LLM fields: `playerName`, `cardName`, `teamName`, `cardNumber`, `numbered`, etc.
4. Deterministic guardrails (if low confidence => unknown state).
5. Teach memory writeback (field corrections + token anchors + context keys).
6. Teach memory replay on next cards (context + token anchor overlap scoring).
7. Human QA + explicit train action.
8. Eval/monitor loop (accuracy, miss rates, drift alarms).

## CardState Contract (Required)
Define one canonical payload that all services consume (`recognition`, `comps`, `inventory`):
1. Identity fields:
- `cardId` (system-generated internal id; not a user-entered card field)
- `manufacturer`
- `setName` (taxonomy dropdown value; keep as canonical set label)
- `setYear` (separate year field used in UI and matching)
- `cardNumber`
- `playerName` (or `cardName` for TCG/non-sports)
2. Optional canonical key field:
- `setId` (recommended stable internal key, e.g. full canonical set identity)
- Note: keep `setYear` separate even if `setId` includes year/manufacturer.
3. Taxonomy fields:
- `insertSet`
- `parallel`
- `autographed` (`true` or `null`)
- `numbered`
- `graded` (`null` or structured grade object)
4. Confidence fields:
- per-field confidence (`0..1`)
- overall confidence (`0..1`)
5. Evidence fields:
- OCR token refs with image side (`FRONT/BACK/TILT`)
- model rationale tags (`in_pool`, `token_match`, `memory_match`, `fallback_used`)
6. Unknown policy:
- any low-confidence taxonomy field must be `unknown`/blank for human review

### Canonical Field Clarifications (UI-Compatible)
1. `cardId`:
- Generated by system (example `tkc_123`).
- Used for tracking/event joins; operators do not type this.
2. `setName` and `setYear`:
- `setYear` maps to the separate Year field in Add Cards.
- `setName` maps to taxonomy dropdown label shown in Add Cards.
- `setName` should not be overloaded with year logic; keep year in `setYear`.
3. `setId`:
- Internal canonical key for storage/join safety.
- Can include year/manufacturer/set identity even when UI `setName` is shorter.
4. `numbered`:
- `null` if not serial-numbered.
- String like `1/5`, `3/25`, `15/99` if serial-numbered.
5. `autographed`:
- `true` if autograph detected/confirmed, else `null`.
6. `graded`:
- `null` for raw cards.
- Object for slabbed cards, e.g. `{ "company": "PSA", "gradeValue": "10", "label": "PSA 10" }`.

### Concrete CardState Example (Raw Card)
```json
{
  "cardId": "tkc_123",
  "manufacturer": "Topps",
  "setYear": "2025-26",
  "setName": "Topps Finest Basketball",
  "setId": "2025-26|Topps|Topps Finest Basketball",
  "playerName": "Devin Vassell",
  "cardNumber": "NL-30",
  "insertSet": "No Limit",
  "parallel": null,
  "numbered": null,
  "autographed": null,
  "graded": null,
  "confidence": {
    "setName": 0.91,
    "insertSet": 0.88,
    "parallel": 0.42,
    "playerName": 0.94,
    "cardNumber": 0.92,
    "numbered": 0.73,
    "autographed": 0.65,
    "graded": 0.99,
    "overall": 0.89
  }
}
```

### Concrete CardState Example (Auto + Numbered + Graded)
```json
{
  "cardId": "tkc_987",
  "manufacturer": "Topps",
  "setYear": "2025-26",
  "setName": "Topps Finest Basketball",
  "setId": "2025-26|Topps|Topps Finest Basketball",
  "playerName": "Cooper Flagg",
  "cardNumber": "80B2-CF",
  "insertSet": "AUTOGRAPH",
  "parallel": "Gold Refractor",
  "numbered": "3/25",
  "autographed": true,
  "graded": {
    "company": "PSA",
    "gradeValue": "10",
    "label": "PSA 10"
  },
  "confidence": {
    "setName": 0.90,
    "insertSet": 0.87,
    "parallel": 0.82,
    "playerName": 0.95,
    "cardNumber": 0.89,
    "numbered": 0.90,
    "autographed": 0.96,
    "graded": 0.93,
    "overall": 0.90
  }
}
```

### Initial Taxonomy Confidence Thresholds (Operator-Approved)
Apply these thresholds only to taxonomy fields (`setName`, `insertSet`, `parallel`):
1. `setName` auto-fill threshold: `0.80`
2. `insertSet` auto-fill threshold: `0.80`
3. `parallel` auto-fill threshold: `0.80`
4. If confidence is below threshold:
- leave field blank (`unknown`) in Add Cards
- require human review/selection
5. Scope note:
- free-text fields (`playerName`, `cardNumber`, `autographed`, `numbered`, etc.) continue to auto-fill from OCR+LLM with their own confidence logic; these thresholds do not block that behavior.

## Learning Event Schema (Always-On Logging)
Log one immutable event per OCR suggestion and per human action:
1. `recognition_suggested`
- card id, batch id, timestamps
- model ids + versions
- candidate pools shown
- suggested CardState + confidences + evidence
2. `recognition_corrected`
- field-level before/after values
- human actor id
- token refs used for correction
3. `recognition_approved`
- final approved CardState
- whether edits were required
4. `comps_suggested` / `comps_corrected` (future phase)
- comp candidate set, selected list, removals, reasons
5. Storage rule:
- never overwrite raw events; derive aggregates/materialized views separately

### Concrete Event JSON Examples
`recognition_suggested`:
```json
{
  "eventType": "recognition_suggested",
  "cardId": "tkc_123",
  "batchId": "batch_20260224_01",
  "model": {
    "ocrProvider": "google_vision_document_text_detection",
    "llmModel": "gpt-5",
    "llmFallbackModel": "gpt-5-mini",
    "promptVersion": "ocr_suggest_v4"
  },
  "candidatePools": {
    "setIds": ["2025-26|Topps|Topps Finest Basketball"],
    "insertSet": ["No Limit", "Daily Dribble", "Rise To Stardom", "The Stars of NBA"],
    "parallel": ["Refractor", "Gold Refractor", "Red Refractor"]
  },
  "suggestedCardState": {
    "setName": "Topps Finest Basketball",
    "setYear": "2025-26",
    "insertSet": "No Limit",
    "parallel": null,
    "playerName": "Devin Vassell",
    "cardNumber": "NL-30",
    "numbered": null,
    "autographed": null,
    "graded": null
  },
  "confidence": {
    "setName": 0.91,
    "insertSet": 0.88,
    "parallel": 0.42
  },
  "evidence": {
    "frontTokens": ["NO", "LIMIT", "NL-30", "DEVIN", "VASSELL"],
    "backTokens": ["TOPPS", "FINEST"],
    "tiltTokens": []
  }
}
```

`recognition_corrected`:
```json
{
  "eventType": "recognition_corrected",
  "cardId": "tkc_123",
  "actorUserId": "usr_42",
  "changes": [
    {
      "field": "parallel",
      "before": null,
      "after": "Refractor"
    }
  ],
  "trainEnabled": true,
  "timestamp": "2026-02-24T18:20:00.000Z"
}
```

## Long-Tail Trigger Definitions (Auto-Queue)
Create a labeled review queue entry when any trigger fires:
1. `set_low_confidence`: `setName` confidence `< 0.80`.
2. `taxonomy_out_of_pool`: insert/parallel suggestion not in canonical pool.
3. `ocr_visual_disagreement`: OCR cue and visual matcher disagree on set/parallel.
4. `memory_conflict`: memory replay disagrees with current high-confidence OCR evidence.
5. `no_comp_high_match` (future comps phase): no sold comp exceeds match threshold.
Each trigger record must include `why`, candidate snapshot, and model versions.

## Three-Speed Learning Policy (SLA)
1. Speed 1: Immediate memory update (seconds)
- write correction event
- update replay/override caches
- effect visible on next similar card in same session
2. Speed 2: Rapid patch training (hourly or every few hours)
- train lightweight judge/ranker on latest reviewed data
- run eval gate before promotion
3. Speed 3: Full retrain (weekly cadence)
- rebuild larger models on full dataset
- publish versioned release notes + metrics delta

### Instant Teach vs Weekly Retrain (No Conflict)
1. Every `Train AI` action should apply immediately:
- save correction + token anchors + optional region markup
- replay on next similar cards in seconds (same set family/layout group)
2. Weekly retrain is still needed:
- it improves the base model itself from many teaches
- it reduces dependence on exact memory matches
- it improves cards the operator has not taught yet
3. Practical analogy:
- instant teach = writing notes the baby brain uses right away
- retrain = upgrading the baby brain's core reading skill from all notes

## Ownership and Incident SLA (Operations)
Use these as default production guardrails:

| Signal | Trigger | Primary Owner | Ack SLA | Mitigation SLA | Rollback Authority |
|---|---|---|---|---|---|
| Wrong-set spike | 30-min rolling wrong-set rate > 8% or >2x 7-day baseline | AI Ops On-Call | 15 min | 60 min | Platform Admin |
| Taxonomy drift | out-of-pool taxonomy suggestion rate > 1% in 30 min | AI Ops On-Call | 15 min | 60 min | Platform Admin |
| Teach replay failure | trained correction not applied on next similar card > 5% sample check | AI Ops On-Call | 30 min | 4 hr | Platform Admin |
| OCR/LLM service degradation | suggestion errors/timeouts > 5% in 15 min | Platform On-Call | 10 min | 30 min | Platform Admin |
| Model/prompt regression after deploy | correction rate worsens > 25% vs prior 24h baseline | Model Owner | 15 min | 60 min | Platform Admin + Product Owner |

### Role Definitions (Minimum)
1. `AI Ops On-Call`: monitors quality dashboard, triages spikes, executes safe mitigation playbook.
2. `Model Owner`: owns prompt/model versioning, rollback recommendation, and eval sign-off.
3. `Platform Admin`: executes rollback and incident commands in production.
4. `Product Owner`: approves policy changes (thresholds, unknown/auto-fill behavior).

## Core Retrain Operations Workflow (UI + Actions)
Goal: keep operator workload low while still improving the core brain globally.

### What the system does automatically
1. Continuously ingests teach signals:
- corrected fields
- approved final values
- region markup templates
2. Builds retrain dataset snapshots on schedule:
- daily lightweight retrain candidate (default)
- weekly full retrain candidate (default)
3. Runs eval gates before any promotion:
- wrong-set rate delta
- out-of-pool rate delta
- human correction rate delta
- unknown-rate stability

### What operators see in AI Ops UI
1. `Production Model` card:
- current model/prompt/memory version
- deployed timestamp
- one-click rollback target
2. `Learning Intake` card:
- teaches since last retrain
- region templates added/updated
- set families touched
3. `Retrain Jobs` table:
- job type (`daily-light`, `weekly-full`)
- dataset snapshot id + size
- status (`queued`, `running`, `passed`, `failed`)
- candidate model version id
4. `Candidate Compare` panel:
- metric deltas versus current production
- pass/fail by gate
- promote/hold actions

### What operator must do
1. During normal card review:
- keep using `Train AI` whenever corrections are made.
2. In AI Ops (once or twice daily):
- review latest candidate compare panel.
- if gates pass and deltas are positive, click `Promote`.
3. If quality regresses:
- click `Rollback` to previous known-good version.
- open incident ticket with trigger and evidence snapshot.

### Promotion policy defaults
1. `daily-light` candidate:
- manual promotion by `Model Owner` or delegated `AI Ops On-Call`.
2. `weekly-full` candidate:
- requires `Model Owner` + `Product Owner` sign-off.
3. Emergency rollback:
- executable by `Platform Admin` immediately.

## Taxonomy Lifecycle Rules (Unknown to Approved)
1. Unknown taxonomy values are allowed at review time but cannot auto-save as canonical labels.
2. New label proposal flow:
- `unknown` observed
- human proposes label
- label enters draft taxonomy state
- approval required before becoming selectable canonical option
3. Alias policy:
- keep alias map per canonical label (e.g., `The Daily Dribble` ↔ `Daily Dribble`)
- matching can use aliases, storage must use canonical value
4. Deprecation policy:
- deprecated labels remain mapped for history; no new records written to deprecated ids

## Release Safety Gates (Model + Prompt + Memory)
No production promotion without all gates passing:
1. Offline eval pass on fixed gold dataset.
2. Canary rollout pass on limited traffic/ops users.
3. Regression checks:
- wrong-set rate
- out-of-pool taxonomy rate
- unknown-rate
- human correction rate
4. Rollback readiness:
- one-command rollback to prior model/prompt/memory config
- previous model artifacts retained and version-addressable

## Master Plan Phases

### Phase 1: Deterministic Candidate-Constrained Selection (Highest ROI)
What to build:
1. Set candidates endpoint for Add Cards using approved sets filtered by year/manufacturer/sport aliases.
2. Once set candidate selected, fetch canonical insert/parallel candidates from DB for that set.
3. Constrain only taxonomy fields (`setName`, `insertSet`, `parallel`) to enumerated candidates (or `unknown`).
4. Keep non-taxonomy fields free-text from OCR+LLM (`playerName`, `cardName`, `cardNumber`, etc.) so we do not require a full player database.
5. Reject out-of-pool labels in API; keep suggestion as `unknown` if no candidate match.
6. Enforce initial taxonomy auto-fill thresholds at `0.80`; below threshold fields remain blank for human review.

Why:
- Removes hallucinated label/set drift.
- Makes model "smart" by using your DB as source of truth.

Definition of done:
1. 0 out-of-pool labels accepted into saved suggestions.
2. Set accuracy materially improves on known test deck.
3. Wrong-set auto-picks drop below agreed threshold.

### Phase 2: Teach Memory v3 (Set-Family Learning)
What to build:
1. Keep writing correction events with context keys (`setId/year/manufacturer/sport/cardNumber`) and token anchors.
2. Add memory aggregation table/materialized view per set family:
- canonical aliases seen in real ops
- field-level confidence priors
- token anchor signatures
3. Replay memory with strict gates:
- set-level replay requires year+manufacturer(+sport where available)
- insert/parallel replay requires set/card context + token overlap

Why:
- Makes "teach one card, learn family" behavior explicit and fast.

Definition of done:
1. After teaching one card in a set family, subsequent cards show improved top-1 suggestion rate in same session.
2. Cross-set contamination rate remains low.

### Phase 3: Unknown-State First (No Forced Guessing)
What to build:
1. Add confidence policy per field:
- if below threshold, set to `unknown` (blank for operator)
2. Add UI reason badges: "unknown because set confidence low", "unknown because out-of-pool".
3. Add one-click teach capture from corrected fields.

Why:
- Better to ask human once than store wrong labels that poison memory.

Definition of done:
1. No silent low-confidence autoselection for set/insert/parallel.
2. Operators can see why field is unknown.

### Phase 4: Region Teach (Core for Set-Family Learning)
What to build:
1. Add "Teach Regions" tool:
- click-drag key text zones on FRONT/BACK/TILT (set logo area, insert title area, card number area)
2. Store region template per set family and layout group.
3. Layout grouping key:
- `setId + layoutClass + photoSide`
- `layoutClass` examples: `base`, `insert_no_limit`, `insert_daily_dribble`, `autograph`, `parallel_refractor`
4. During OCR replay, prioritize tokens from taught regions for matching and confidence scoring.
5. One-teach-many behavior:
- when operator teaches one base card with markup, apply same template immediately to other base cards in that set family.
- keep insert/autograph/parallel templates separate to avoid cross-layout contamination.

Why:
- Gives explicit visual grounding when text alone is ambiguous.
- Enables "teach once, apply to next similar cards immediately."

Definition of done:
1. After one base-card region teach, next 20 base cards in same set family show measurable accuracy lift.
2. Region templates do not leak across incompatible layout classes.
3. Tool remains fast (target under 10 seconds per teach).

### Phase 5: Multimodal LLM Upgrade Path (Selective)
What to build:
1. For hard cards only, send image + OCR text to model for disambiguation.
2. Use `detail: high` only when needed to control cost/latency.
3. Keep candidate-constrained output schema mandatory.

Why:
- Some inserts/parallels are visually distinct but text-similar.

Definition of done:
1. Hard-case accuracy improves without unacceptable cost increase.
2. Latency SLA remains acceptable for Add Cards.

### Phase 6: Eval Flywheel and Drift Control
What to build:
1. Gold test set per major product line (base + inserts + parallels + difficult negatives).
2. Weekly eval runs and release gate:
- set accuracy
- insert/parallel top-1 and top-3
- unknown-rate
- wrong-set rate
- cross-set memory drift
3. Trace/eval dashboard with pass/fail thresholds.

Why:
- Prevents regressions and creates predictable quality gains.

Definition of done:
1. No prod release without eval pass.
2. Trendlines visible for each metric.

## Operator Teaching SOP (What Humans Should Do)
1. If suggestion is correct: keep it, continue.
2. If suggestion is wrong:
- correct set/insert/parallel/card fields
- keep `Train AI` on
- send forward normally
3. If uncertain card type:
- leave unknown fields blank instead of forcing a guess
- complete manually and train
4. Use region teach when:
- first card of a new set family is encountered
- recurring miss pattern appears in same set family/layout
5. For base-card training:
- teach one representative base card with region markup so template applies to other base cards in same set family.

## Data Model Extensions (Recommended)
1. `SetLearningProfile` (new)
- `setId`, `year`, `manufacturer`, `sport`
- alias maps for set/insert/parallel
- confidence priors by field
- updated timestamps and counters
2. `SetRegionTemplate` (core)
- per set + layout class + photo side + labeled bounding regions
- versioned templates with createdBy/updatedAt/audit note
3. keep `OcrFeedbackEvent` as raw event log of truth corrections

## API/Service Changes (Recommended)
1. `GET /api/admin/variants/options`
- returns stable canonical candidate pools + IDs
2. `POST /api/admin/cards/:id/ocr-suggest`
- receives candidate pools and must output enum-safe labels or `unknown`
3. `POST /api/admin/cards/:id/train` (optional explicit endpoint)
- captures teach action + optional region data

## Guardrails
1. Never auto-save non-canonical option labels.
2. Never auto-select set from weak single-token hints.
3. Memory replay must pass context gates.
4. If confidence low, return unknown and ask human.

## Metrics (Release Gates)
1. Set top-1 accuracy
2. Insert/parallel top-1 and top-3 accuracy
3. Unknown-rate (should be controlled, not zero)
4. Wrong-set rate
5. Human correction rate after suggestion
6. Memory lift: accuracy before/after teach in same set family

## Rollout Sequence
1. Phase 1 (candidate-constrained selection) + gate metrics
2. Phase 2 (memory v3) + memory lift tracking
3. Phase 4 (region teach core rollout)
4. Phase 3 (unknown-first UX)
5. Phase 6 (eval gate mandatory)
6. Phase 5 as selective advanced upgrade

## Research Basis (Primary Sources)
1. OpenAI Responses API (text/image input, structured outputs, tools):
- https://platform.openai.com/docs/api-reference/responses
2. OpenAI Structured Outputs (`json_schema` vs `json_object`):
- https://developers.openai.com/api/docs/guides/structured-outputs
3. OpenAI vision/image input guidance (`detail` low/high/auto):
- https://developers.openai.com/api/docs/guides/images-vision
4. OpenAI GPT-5 model docs (modalities, reasoning controls, endpoint support):
- https://developers.openai.com/api/docs/models/gpt-5
5. OpenAI evaluation best practices and eval tooling:
- https://developers.openai.com/api/docs/guides/evaluation-best-practices
- https://developers.openai.com/api/reference/resources/evals
- https://developers.openai.com/api/docs/guides/agent-evals
6. Google Cloud Vision OCR overview (`TEXT_DETECTION` vs `DOCUMENT_TEXT_DETECTION`):
- https://docs.cloud.google.com/vision/docs/ocr
7. Google full text annotation hierarchy (Page→Block→Paragraph→Word→Symbol):
- https://docs.cloud.google.com/vision/docs/fulltext-annotations
8. Google guidance on language hints (use carefully; empty often best for Latin OCR):
- https://docs.cloud.google.com/vision/docs/ocr

## Notes for Future Codex Agents
1. Do not replace candidate-constrained logic with free-text label generation.
2. Preserve unknown-state behavior when confidence is weak.
3. Every model/prompt change must be eval-gated against the same gold dataset.
4. Prefer additive memory improvements over broad heuristics.
